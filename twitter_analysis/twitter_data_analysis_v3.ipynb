{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syria Twitter Data Processing, Visualization, and NLP Analysis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import core libraries \n",
    "import datetime\n",
    "import json\n",
    "import csv\n",
    "import ast\n",
    "import pathlib\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "\n",
    "# import third-party libraries\n",
    "import lxml.html\n",
    "import pandas\n",
    "from pandas.io.json import json_normalize\n",
    "from pandas import ExcelWriter\n",
    "\n",
    "# import visualizations\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new line json streamer for processing the data -> MAIN FUNCTION FOR ANALYSIS\n",
    "# json objects must be stored per line, not in an arrary \n",
    "# my dataset contains separate JSON object on each line\n",
    "def nljson_generator(json_path):\n",
    "    with open(json_path) as file:\n",
    "        for line in file: \n",
    "            yield json.loads(line)\n",
    "\n",
    "# get the total number of tweet json objects in dataset\n",
    "def count_tweet_objects(json_path):\n",
    "    count = 0\n",
    "    for tweet in nljson_generator(json_path): \n",
    "        count+=1\n",
    "    return count\n",
    "\n",
    "# read n number of json objects from the tweets dataset\n",
    "def get_n_tweets(json_path, n_tweets):\n",
    "    data = []\n",
    "    for line in islice(nljson_generator(json_path), n_tweets):\n",
    "        data.append(line)\n",
    "    return(data)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory path data\n",
    "twitter_data_dir = pathlib.Path('/Users/adamstueckrath/Desktop/twitter_data/')\n",
    "\n",
    "# set tweets_no_rts_json\n",
    "tweets_no_rts_json = twitter_data_dir / 'tweets_no_retweets' / 'tweets_no_retweets.json'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify tweet attributes for analysis\n",
    "I'm going identify all important attributes for this project and list them below. This will be a part of the data cleaning/processing section. I referenced the Twitter API official documentation to understand all of the fields and identify what information I want to pull from each tweet. Here's the link: https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object\n",
    "\n",
    "**Twitter Tweet Attributes** <br /> \n",
    "Tweets are the basic atomic building block of all things Twitter. Tweets are also known as “status updates.” \n",
    "* `created_at`: UTC time when this Tweet was created.\n",
    "* `id`: The integer representation of the unique identifier for this Tweet. \n",
    "* `id_str`: The string representation of the unique identifier for this Tweet.\n",
    "* `text`: The actual UTF-8 text of the tweet.\n",
    "* `source`: Utility used to post the Tweet as an HTML-formatted string.\n",
    "* `retweet_count`: Number of times the tweet was retweeted.\n",
    "* `favorite_count`: Number of times the tweet was favorited.\n",
    "* `lang`: When present, indicates a BCP 47 language identifier corresponding to the machine-detected language of the Tweet text, or `und` if no language could be detected. \n",
    "* `coordinates`: Represents the geographic location of this Tweet as reported by the user or client application.\n",
    "* `geo`: This deprecated attribute has its coordinates formatted as [lat, long], while all other Tweet geo is formatted as [long, lat].\n",
    "* `place`: When present, indicates that the tweet is associated (but not necessarily originating from)\n",
    "\n",
    "**Twitter User Attributes** <br />\n",
    "The `user` object contains public Twitter account metadata and describes the account.\n",
    "* `name`: The name of the user, as they’ve defined it.\n",
    "* `screen_name`: The screen name, handle, or alias that this user identifies themselves with.\n",
    "* `location`: The user-defined location for this account’s profile. \n",
    "* `verified`: When true, indicates that the user has a verified account.\n",
    "* `followers_count`: The number of followers this account currently has. \n",
    "* `utc_offset`: To calculate the time relative to the user's timezone.\n",
    "\n",
    "**Twitter Tweet Entities Attributes** <br /> \n",
    "The `entities` section provides arrays of common things included in Tweets: hashtags, user mentions, links, stock tickers (symbols), Twitter polls, and attached media.\n",
    "* `hashtags`: Represents hashtags which have been parsed out of the Tweet text. (e.g. \"#Syria\" appears as \"Syria\")\n",
    "* `user_mentions`: Represents other Twitter users mentioned in the text of the Tweet. \n",
    "* `media`: Represents media elements uploaded with the Tweet. \n",
    "* `type`: The actual type of media is specified in the media.\n",
    "* `url`: the expanded version of urls included in the tweet (e.g. \"https://t.co/ljRAxRICTr\" is the shortened URL in the tweet and the full url is https://www.nytimes.com/reuters/2017/03/21/world/middleeast/21reuters-israel-syria-iran.html)\n",
    "* `title`: HTML title for the link.\n",
    "* `description`: HTML description for the link.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning functions \n",
    "def clean_hashtags(tweet_hashtags):\n",
    "    \"\"\"\n",
    "    Turns data with any number of hashtags like this:\n",
    "    'hashtags': [{'text': 'FAKENEWS', 'indices': [80, 89]}]\n",
    "    to a list like this -> ['FAKENEWS']\n",
    "    \"\"\"\n",
    "    hashtags_cleaned = []\n",
    "    if len(tweet_hashtags) >= 1:\n",
    "        for tag in range(len(tweet_hashtags)):\n",
    "            hashtag_text = tweet_hashtags[tag]['text'].lower()\n",
    "            hashtags_cleaned.append(hashtag_text)\n",
    "    return hashtags_cleaned\n",
    "\n",
    "def clean_source(source):\n",
    "    \"\"\"\n",
    "    Turns data including the source and some html like this:\n",
    "    <a href=\"http://twitter.com/download/android\" rel=\"nofollow\">Twitter for Android</a> \n",
    "    to a string like this -> 'Twitter for Android'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        raw = lxml.html.document_fromstring(source)\n",
    "        raw = raw.text_content()\n",
    "    except:\n",
    "        return None\n",
    "    return raw\n",
    "\n",
    "def string_to_datetime(tweet_date):\n",
    "    \"\"\"\n",
    "    Turns a datetime string like this: \n",
    "    '2017-07-06T18:34:37.000Z' \n",
    "    to a Python datetime object like this -> 2017-07-06 18:34:41\n",
    "    \"\"\"\n",
    "    return datetime.datetime.strptime(tweet_date, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n",
    "\n",
    "def clean_user_mentions(user_mentions):\n",
    "    \"\"\"\n",
    "    Turns data like this:\n",
    "    [{'screen_name': 'TheSwogBlog', 'name': 'The Swog Blog', 'id': 7.130089490967429e+17, \n",
    "    'id_str': '713008949096742912', 'indices': [0, 12]}]\n",
    "    into to a list -> ['TheSwogBlog']\n",
    "    \"\"\"\n",
    "    user_mentions_cleaned = []\n",
    "    if len(user_mentions) >= 1:\n",
    "        for user in range(len(user_mentions)):\n",
    "            mention = user_mentions[user]['screen_name']\n",
    "            user_mentions_cleaned.append(mention)\n",
    "    return user_mentions_cleaned\n",
    "\n",
    "def clean_geo_and_coordinates(tweet_geo_or_coordinates):\n",
    "    \"\"\"\n",
    "    Extracts elements of a dictionary like:  \n",
    "    {'type': 'Point', 'coordinates': [35.3612, 31.3893]}\n",
    "    into a list like this -> [35.3612, 31.3893]\n",
    "    \"\"\"\n",
    "    tweet_coordinates = None\n",
    "    if tweet_geo_or_coordinates:\n",
    "        tweet_coordinates = tweet_geo_or_coordinates['coordinates']\n",
    "    return tweet_coordinates\n",
    "\n",
    "def clean_places(tweet_place):\n",
    "    \"\"\"\n",
    "    Extracts elements of a dictionary like:  \n",
    "    {'id': '65b23b0045f450f6', 'url': 'https://api.twitter.com/1.1/geo/id/65b23b0045f450f6.json', \n",
    "    'place_type': 'city', 'name': 'Kingston upon Thames', 'full_name': 'Kingston upon Thames, London', \n",
    "    'country_code': 'GB', 'country': 'United Kingdom', \n",
    "    'bounding_box': {'type': 'Polygon', 'coordinates': [[[-0.322917, 51.34286], \n",
    "    [-0.322917, 51.437266], [-0.234011, 51.437266], [-0.234011, 51.34286]]]}, 'attributes': {}}\n",
    "    \n",
    "    Returns a tuple of the dictionary elements: \n",
    "    ('city', 'Kingston upon Thames', 'Kingston upon Thames, London', 'GB', 'United Kingdom', 'Polygon', \n",
    "    [[-0.322917, 51.34286], [-0.322917, 51.437266], [-0.234011, 51.437266], [-0.234011, 51.34286]])\n",
    "    \n",
    "    \"\"\"\n",
    "    place_type = name = full_name = country_code = None\n",
    "    country = bounding_box_type = bounding_box_coordinates = None\n",
    "\n",
    "    if tweet_place:\n",
    "        place_type = tweet_place['place_type']\n",
    "        name = tweet_place['name']\n",
    "        full_name = tweet_place['full_name']\n",
    "        country_code = tweet_place['country_code']\n",
    "        country = tweet_place['country']\n",
    "        bounding_box_type = tweet_place['bounding_box']['type']\n",
    "        bounding_box_coordinates = tweet_place['bounding_box']['coordinates'][0]\n",
    "    \n",
    "    return place_type, name, full_name, country_code, \\\n",
    "           country, bounding_box_type, bounding_box_coordinates\n",
    "\n",
    "def clean_entities_url(tweet_entities):\n",
    "    \"\"\"\n",
    "    Extracts the expanded url from a dictionary like:  \n",
    "    [{'url': 'https://t.co/Eiqt4Gu4hs', 'expanded_url': 'https://twitter.com/i/web/status/883031529303232512', \n",
    "    'display_url': 'twitter.com/i/web/status/8…', 'indices': [121, 144]}]\n",
    "    To a string like this -> 'https://twitter.com/i/web/status/883031529303232512'\n",
    "    \"\"\"\n",
    "    tweet_entities_urls_expanded_url = None\n",
    "    if tweet_entities['urls']:\n",
    "        tweet_entities_urls_expanded_url = tweet_entities['urls'][0]['expanded_url']\n",
    "    return tweet_entities_urls_expanded_url\n",
    "\n",
    "def clean_extended_entities(tweet):\n",
    "    \"\"\"\n",
    "    Extracts elements of the tweet extended_entities attribute like:  \n",
    "    {'expanded_url': 'https://twitter.com/Reuters/status/883028019266281472/video/1', 'type': 'video'}\n",
    "    \"\"\"\n",
    "    tweet_extended_entities_media_type = None\n",
    "    tweet_extended_entities_media_url = None\n",
    "    tweet_extended_entities = tweet.get('extended_entities', None)\n",
    "    if tweet_extended_entities:\n",
    "        tweet_entended_entities_media = tweet_extended_entities.get('media', None)\n",
    "        if tweet_entended_entities_media:\n",
    "            tweet_extended_entities_media_type = tweet_entended_entities_media[0]['type']       \n",
    "            tweet_extended_entities_media_url = tweet_entended_entities_media[0]['expanded_url']\n",
    "    return tweet_extended_entities_media_type, tweet_extended_entities_media_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# write twitter data to flatten csv file\n",
    "def write_tweets_to_csv(tweets_json, outfile):\n",
    "    # the headers are the fields that we identified in step 4\n",
    "    headers = ['tweet_id', 'tweet_id_str', 'tweet_created_at', 'tweet_geo', 'tweet_coordinates', \n",
    "               'place_type', 'place_name', 'place_full_name', 'place_country', 'place_country_code',\n",
    "               'bounding_box_type', 'bounding_box_coordinates', 'tweet_lang', \n",
    "               'tweet_source', 'tweet_text', 'tweet_retweet_count', 'tweet_favorite_count', \n",
    "               'user_id_str', 'user_screen_name', 'user_name', 'user_location', 'user_utc_offset', \n",
    "               'user_verified', 'user_followers_count', 'tweet_hashtags', 'tweet_user_mentions', \n",
    "               'tweet_expanded_url', 'tweet_media_type', 'tweet_media_url']\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_no_rts_csv file path\n",
    "tweets_no_rts_csv = twitter_data_dir / 'tweets_no_retweets' / 'tweets_no_retweets.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tweets into dataframe from csv file\n",
    "tweets_no_rts_df = pandas.read_csv(tweets_no_rts_csv, header=0,\n",
    "                                   parse_dates=['tweet_created_at'], \n",
    "                                   date_parser=string_to_datetime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1160088, 29)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print dataframe\n",
    "tweets_no_rts_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['russian', 'warplanes', 'bombed', 'the', 'town', 'of', 'suha', 'in', 'hama', 'province', 'neither', 'fatalities', 'nor', 'injuries', 'are', 'reported']\n"
     ]
    }
   ],
   "source": [
    "test = 'Russian warplanes bombed the town of Suha in Hama province Neither fatalities nor injuries are reported'\n",
    "event_list = test.lower().split()\n",
    "print(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    Utility function to clean the text in a tweet by removing \n",
    "    links and special characters using regex.\n",
    "    '''\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "def check_tweet(tweet):\n",
    "    \n",
    "    tweet = clean_tweet(tweet.lower())\n",
    "    tweet = tweet.split()\n",
    "    for word in tweet: \n",
    "        if word in event_list:\n",
    "            return True\n",
    "        return False \n",
    "    \n",
    "tweets_no_rts_df['Test'] = tweets_no_rts_df[\"tweet_text\"].apply(lambda tweet: check_tweet(tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-06\n"
     ]
    }
   ],
   "source": [
    "event_date = pandas.to_datetime('2017-07-06').date()\n",
    "print(event_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(tweet_date):\n",
    "    if tweet_date.date() == event_date:\n",
    "        return True\n",
    "    return False \n",
    "\n",
    "tweets_no_rts_df['Test_Date'] = tweets_no_rts_df[\"tweet_created_at\"].apply(lambda tweet: testing(tweet))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    1153599\n",
       "True        6489\n",
       "Name: Test_Date, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_no_rts_df['Test_Date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = tweets_no_rts_df[(tweets_no_rts_df.Test_Date == True) & (tweets_no_rts_df.Test == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.to_csv('/Users/adamstueckrath/Desktop/twitter_data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-4cbccafe277d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets_no_rts_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweets_no_rts_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Test_Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mtweets_no_rts_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Test'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/syria-project-3.6.5/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1574\u001b[0m         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n\u001b[1;32m   1575\u001b[0m                          \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1576\u001b[0;31m                          .format(self.__class__.__name__))\n\u001b[0m\u001b[1;32m   1577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "test_df = tweets_no_rts_df[tweets_no_rts_df['Test_Date'] == True & tweets_no_rts_df['Test'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Tweet hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top hashtags\n",
    "top_hashtags_df = pandas.DataFrame(tweets_no_rts_df, \n",
    "                                   columns=['tweet_hashtags'])\n",
    "top_hashtags_df['tweet_hashtags'] = top_hashtags_df['tweet_hashtags'].apply(lambda x: ast.literal_eval(x))\n",
    "top_hashtags_list = top_hashtags_df['tweet_hashtags'].tolist()\n",
    "top_hashtags_list = list(itertools.chain.from_iterable(top_hashtags_list))\n",
    "top_n_hashtags = Counter(top_hashtags_list).most_common(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe of top hashtags to chart\n",
    "#  top_hashtags_df = pandas.DataFrame(top_n_hashtags, columns=['hashtag', 'hashtag_count'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Media Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    Utility function to clean the text in a tweet by removing \n",
    "    links and special characters using regex.\n",
    "    '''\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "def analize_sentiment(tweet):\n",
    "    '''\n",
    "    Utility function to classify the polarity of a tweet\n",
    "    using textblob.\n",
    "    '''\n",
    "    analysis = TextBlob(clean_tweet(tweet))\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 1\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sentiment of english tweets\n",
    "tweet_sentiment_analysis_df = tweets_no_rts_df[(tweets_no_rts_df.tweet_lang == 'en')]\n",
    "tweet_sentiment_analysis_df = tweet_sentiment_analysis_df.reset_index(drop=True) \n",
    "tweet_sentiment_analysis_df['SentimentAnalysis'] = tweet_sentiment_analysis_df['tweet_text'].apply(analize_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of positive tweets: 28%\n",
      "Percentage of neutral tweets: 52%\n",
      "Percentage de negative tweets: 20%\n"
     ]
    }
   ],
   "source": [
    "# print sentiment analysis values\n",
    "total_values = len(tweet_sentiment_analysis_df.index)\n",
    "sentiment_values = tweet_sentiment_analysis_df['SentimentAnalysis'].value_counts().to_dict()\n",
    "positive_tweets = sentiment_values[1]\n",
    "neutral_tweets = sentiment_values[0]\n",
    "negative_tweets = sentiment_values[-1]\n",
    "print(\"Percentage of positive tweets: {}%\".format(round(positive_tweets*100/total_values)))\n",
    "print(\"Percentage of neutral tweets: {}%\".format(round(neutral_tweets*100/total_values)))\n",
    "print(\"Percentage de negative tweets: {}%\".format(round(negative_tweets*100/total_values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_values['Posivite'] = sentiment_values.pop(1)\n",
    "sentiment_values['Neutral'] = sentiment_values.pop(0)\n",
    "sentiment_values['Negative'] = sentiment_values.pop(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more accurate results I should consider adding the retweets back into my dataset, but dropping any duplicates. An interesting idea would be to analyze the polarity of the tweets from different media types. It might be deterministic that by only considering the tweets from a specific media type, the polarity would result more positive/negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps in my analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
