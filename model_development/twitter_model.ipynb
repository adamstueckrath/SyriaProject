{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import core libraries \n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import ast\n",
    "import pathlib\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "\n",
    "# import third-party libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "from pandas import ExcelWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory path data\n",
    "twitter_data_dir = pathlib.Path('/Users/adamstueckrath/Desktop/syria_data/')\n",
    "\n",
    "# tweets_no_rts_csv file path\n",
    "tweets_no_rts_csv = twitter_data_dir / 'tweets_no_retweets' / 'tweets_no_retweets.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_datetime(tweet_date):\n",
    "    \"\"\"\n",
    "    Turns a datetime string like this: \n",
    "    '2017-07-06T18:34:37.000Z' \n",
    "    to a Python datetime object like this -> 2017-07-06 18:34:41\n",
    "    \"\"\"\n",
    "    return datetime.datetime.strptime(tweet_date, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tweets into dataframe from csv file\n",
    "tweets_no_rts_df = pd.read_csv(tweets_no_rts_csv, header=0,\n",
    "                               parse_dates=['tweet_created_at'], \n",
    "                               date_parser=string_to_datetime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamstueckrath/.pyenv/versions/3.6.5/envs/syria-project-3.6.5/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    Utility function to clean the text in a tweet by removing \n",
    "    links and special characters using regex.\n",
    "    '''\n",
    "    tweet = tweet.lower()\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "def analyze_sentiment(tweet):\n",
    "    '''\n",
    "    Utility function to classify the polarity of a tweet\n",
    "    using nltk. analysis variable returns the following dict: \n",
    "    {'neg': 0.122, 'neu': 0.641, 'pos': 0.237, 'compound': 0.4215}\n",
    "    The compound value here conveys the overall positive or negative user experience.\n",
    "    Examples: \n",
    "    https://www.programcreek.com/python/example/100005/nltk.sentiment.vader.SentimentIntensityAnalyzer\n",
    "    https://opensourceforu.com/2016/12/analysing-sentiments-nltk/\n",
    "    '''\n",
    "    analysis = analyzer.polarity_scores(clean_tweet(tweet))\n",
    "    if analysis['compound'] > 0.1:\n",
    "        return 1\n",
    "    elif analysis['compound'] == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'Assad loses 3 generals during the first week of July #Syria #RevolutionWins #militia #Free_Syrian_Army\\nhttps://t.co/5x8UbVOnH9'\n",
    "\n",
    "testing = clean_tweet(test)\n",
    "print(testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_no_rts_df_en = tweets_no_rts_df.copy()\n",
    "tweets_no_rts_df_en = tweets_no_rts_df_en[tweets_no_rts_df_en['tweet_lang'] =='en']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_no_rts_df_en['tweet_text_clean'] = tweets_no_rts_df_en['tweet_text'].apply(clean_tweet)\n",
    "tweets_no_rts_df_en['tweet_text_sentiment'] = tweets_no_rts_df_en['tweet_text_clean'].apply(analyze_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tweets_no_rts_df_en['tweet_text_sentiment'].value_counts().to_dict()\n",
    "print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "638161\n"
     ]
    }
   ],
   "source": [
    "tweet_text = tweets_no_rts_df_en['tweet_text_clean']\n",
    "tweet_text_list = tweet_text.tolist()\n",
    "print(len(tweet_text_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What characterizes text of different sentiments?\n",
    "While we still haven't decided what classification method to use, it's useful to get an idea of how the different texts look. This might be an \"old school\" approach in the age of deep learning, but lets indulge ourselves nevertheless.\n",
    "\n",
    "To explore the data we apply some crude preprocessing. We will tokenize and lemmatize using Python NLTK, and transform to lower case. As words mostly matter in context we'll look at bi-grams instead of just individual tokens.\n",
    "\n",
    "As a way to simplify later inspection of results we will store all processing of data together with it's original form. This means we will extend the Pandas dataframe into which we imported the raw data with new columns as we go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def normalizer(tweet):\n",
    "    only_letters = re.sub(\"[^a-zA-Z]\", \" \", tweet) \n",
    "    tokens = nltk.word_tokenize(only_letters)\n",
    "    lower_case = [l.lower() for l in tokens]\n",
    "    filtered_result = list(filter(lambda l: l not in stop_words, lower_case))\n",
    "    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]\n",
    "    return lemmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_no_rts_df_en['tweet_text_normalized'] = tweets_no_rts_df_en['tweet_text_clean'].apply(normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "def ngrams(input_list):\n",
    "    #onegrams = input_list\n",
    "    bigrams = [' '.join(t) for t in list(zip(input_list, input_list[1:]))]\n",
    "    trigrams = [' '.join(t) for t in list(zip(input_list, input_list[1:], input_list[2:]))]\n",
    "    return bigrams+trigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_no_rts_df_en['tweet_text_grams'] = tweets_no_rts_df_en['tweet_text_normalized'].apply(ngrams)\n",
    "tweets_no_rts_df_en[['tweet_text_grams']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(input):\n",
    "    cnt = Counter()\n",
    "    for row in input:\n",
    "        for word in row:\n",
    "            cnt[word] += 1\n",
    "    return cnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_no_rts_df_en[(tweets_no_rts_df_en.tweet_text_sentiment == 1)][['tweet_text_grams']].apply(count_words)['tweet_text_grams'].most_common(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_no_rts_df_en[(tweets_no_rts_df_en.tweet_text_sentiment == -1)][['tweet_text_grams']].apply(count_words)['tweet_text_grams'].most_common(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM classifier\n",
    "We will build a simple, linear Support-Vector-Machine (SVM) classifier. The classifier will take into account each unique word present in the sentence, as well as all consecutive words. To make this representation useful for our SVM classifier we transform each sentence into a vector. The vector is of the same length as our vocabulary, i.e. the list of all words observed in our training data, with each word representing an entry in the vector. If a particular word is present, that entry in the vector is 1, otherwise 0.\n",
    "\n",
    "To create these vectors we use the CountVectorizer from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tweets_no_rts_df_en.copy()[:50000]\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = count_vectorizer.fit_transform(test.tweet_text)\n",
    "indexed_data = hstack((np.array(range(0,vectorized_data.shape[0]))[:,None], vectorized_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment2target(sentiment):\n",
    "    return {\n",
    "        -1: 0,\n",
    "        0: 1,\n",
    "        1: 2\n",
    "    }[sentiment]\n",
    "targets = test.tweet_text_sentiment.apply(sentiment2target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test, targets_train, targets_test = train_test_split(indexed_data, targets, test_size=0.4, random_state=0)\n",
    "data_train_index = data_train[:,0]\n",
    "data_train = data_train[:,1:]\n",
    "data_test_index = data_test[:,0]\n",
    "data_test = data_test[:,1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "clf = OneVsRestClassifier(svm.SVC(gamma=0.01, C=100., probability=True, class_weight='balanced', kernel='linear'))\n",
    "clf_output = clf.fit(data_train, targets_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(data_test, targets_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering tweet words (for feature vector)\n",
    "Stop words - a, is, the, with etc. The full list of stop words can be found at Stop Word List. These words don't indicate any sentiment and can be removed.\n",
    "\n",
    "Repeating letters - if you look at the tweets, sometimes people repeat letters to stress the emotion. E.g. hunggrryyy, huuuuuuungry for 'hungry'. We can look for 2 or more repetitive letters in words and replace them by 2 of the same.\n",
    "\n",
    "Punctuation - we can remove punctuation such as comma, single/double quote, question marks at the start and end of each word. E.g. beautiful!!!!!! replaced with beautiful\n",
    "\n",
    "Words must start with an alphabet - For simplicity sake, we can remove all those words which don't start with an alphabet. E.g. 15th, 5.34am\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=25, n_init=1, n_jobs=None, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(tweet_text_list)\n",
    "\n",
    "true_k = 25\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " syria\n",
      " russia\n",
      " trump\n",
      " iran\n",
      " israel\n",
      " assad\n",
      " like\n",
      " just\n",
      " people\n",
      " good\n",
      "Cluster 1:\n",
      " killed\n",
      " civilians\n",
      " fighting\n",
      " syria\n",
      " coalition\n",
      " isis\n",
      " 600\n",
      " strikes\n",
      " iraq\n",
      " british\n",
      "Cluster 2:\n",
      " asimah\n",
      " kuwait\n",
      " st\n",
      " city\n",
      " al\n",
      " damascus\n",
      " alen56\n",
      " bk8\n",
      " q8\n",
      " 218\n",
      "Cluster 3:\n",
      " isis\n",
      " syria\n",
      " fight\n",
      " lgbt\n",
      " fighting\n",
      " women\n",
      " unit\n",
      " burqas\n",
      " queer\n",
      " created\n",
      "Cluster 4:\n",
      " al\n",
      " qaeda\n",
      " syria\n",
      " idlib\n",
      " sham\n",
      " cnn\n",
      " propagandist\n",
      " hired\n",
      " qaida\n",
      " documentary\n",
      "Cluster 5:\n",
      " usa\n",
      " russia\n",
      " syria\n",
      " uk\n",
      " politics\n",
      " nato\n",
      " israel\n",
      " china\n",
      " eu\n",
      " trump\n",
      "Cluster 6:\n",
      " putin\n",
      " trump\n",
      " syria\n",
      " ceasefire\n",
      " russia\n",
      " meeting\n",
      " deal\n",
      " signs\n",
      " cease\n",
      " assad\n",
      "Cluster 7:\n",
      " damascus\n",
      " knife\n",
      " custom\n",
      " steel\n",
      " hunting\n",
      " handmade\n",
      " syrian\n",
      " near\n",
      " handle\n",
      " strike\n",
      "Cluster 8:\n",
      " war\n",
      " syria\n",
      " civil\n",
      " world\n",
      " crimes\n",
      " russia\n",
      " israel\n",
      " trump\n",
      " torn\n",
      " iran\n",
      "Cluster 9:\n",
      " says\n",
      " syria\n",
      " russia\n",
      " official\n",
      " trump\n",
      " war\n",
      " state\n",
      " military\n",
      " russian\n",
      " coalition\n",
      "Cluster 10:\n",
      " lebanon\n",
      " border\n",
      " hezbollah\n",
      " syria\n",
      " operation\n",
      " launches\n",
      " thousands\n",
      " lebanese\n",
      " offensive\n",
      " launch\n",
      "Cluster 11:\n",
      " cease\n",
      " syria\n",
      " reach\n",
      " russia\n",
      " deal\n",
      " effect\n",
      " southern\n",
      " goes\n",
      " southwest\n",
      " ap\n",
      "Cluster 12:\n",
      " iraq\n",
      " syria\n",
      " libya\n",
      " isis\n",
      " yemen\n",
      " iran\n",
      " afghanistan\n",
      " daesh\n",
      " isil\n",
      " amp\n",
      "Cluster 13:\n",
      " focus\n",
      " round\n",
      " terrorism\n",
      " talks\n",
      " fight\n",
      " ends\n",
      " syria\n",
      " reuters\n",
      " news\n",
      " international\n",
      "Cluster 14:\n",
      " raqqa\n",
      " backed\n",
      " forces\n",
      " isis\n",
      " sdf\n",
      " fighters\n",
      " syrian\n",
      " daesh\n",
      " battle\n",
      " city\n",
      "Cluster 15:\n",
      " army\n",
      " syrian\n",
      " eastern\n",
      " ghouta\n",
      " raqqa\n",
      " east\n",
      " islamic\n",
      " state\n",
      " observatory\n",
      " oil\n",
      "Cluster 16:\n",
      " cia\n",
      " program\n",
      " trump\n",
      " covert\n",
      " syria\n",
      " end\n",
      " rebels\n",
      " ending\n",
      " support\n",
      " confirms\n",
      "Cluster 17:\n",
      " news\n",
      " syria\n",
      " fake\n",
      " truth\n",
      " bbc\n",
      " russia\n",
      " latest\n",
      " breaking\n",
      " video\n",
      " fox\n",
      "Cluster 18:\n",
      " chemical\n",
      " attack\n",
      " weapons\n",
      " syria\n",
      " targets\n",
      " held\n",
      " area\n",
      " sdf\n",
      " daesh\n",
      " use\n",
      "Cluster 19:\n",
      " ceasefire\n",
      " russia\n",
      " deal\n",
      " southwest\n",
      " syria\n",
      " agree\n",
      " brokered\n",
      " begins\n",
      " effect\n",
      " russian\n",
      "Cluster 20:\n",
      " ends\n",
      " anti\n",
      " rebels\n",
      " cia\n",
      " assad\n",
      " arm\n",
      " trump\n",
      " covert\n",
      " program\n",
      " sought\n",
      "Cluster 21:\n",
      " amp\n",
      " syria\n",
      " iran\n",
      " russia\n",
      " assad\n",
      " iraq\n",
      " isis\n",
      " trump\n",
      " obama\n",
      " people\n",
      "Cluster 22:\n",
      " troops\n",
      " locations\n",
      " leaks\n",
      " stronghold\n",
      " inside\n",
      " official\n",
      " secret\n",
      " turkey\n",
      " daesh\n",
      " syria\n",
      "Cluster 23:\n",
      " petition\n",
      " sign\n",
      " planes\n",
      " nowwiii\n",
      " war\n",
      " rt\n",
      " syria\n",
      " clash\n",
      " stop\n",
      " russia\n",
      "Cluster 24:\n",
      " aleppo\n",
      " animals\n",
      " zoo\n",
      " rescued\n",
      " syria\n",
      " syrian\n",
      " rebuilding\n",
      " historic\n",
      " hotel\n",
      " nostalgia\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prediction\n",
      "[7]\n",
      "[7]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "print(\"Prediction\")\n",
    "\n",
    "Y = vectorizer.transform([\"Trump 3 generals dead during the first week of July is toast Trump\"])\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)\n",
    "\n",
    "Y = vectorizer.transform([\"Help trump is president\"])\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
