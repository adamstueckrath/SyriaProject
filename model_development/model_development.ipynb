{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import core libraries \n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import ast\n",
    "import pprint\n",
    "import pathlib\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "\n",
    "# import third-party libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import visualizations\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory path data\n",
    "syria_data_dir = pathlib.Path('/Users/adamstueckrath/Desktop/syria_data/')\n",
    "\n",
    "# syria_events_csv file path\n",
    "events_pre_processed_csv = syria_data_dir / 'model' / 'model_data' / 'events_pre_processed.csv'\n",
    "\n",
    "# tweets_no_rts_csv file path\n",
    "tweets_pre_processed_csv = syria_data_dir / 'model' / 'model_data' / 'tweets_pre_processed.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv(tweets_pre_processed_csv, header=0)\n",
    "events_df = pd.read_csv(events_pre_processed_csv, header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tweets_df.dropna(subset=['tweet_text_normalize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>event_date</th>\n",
       "      <th>event_type</th>\n",
       "      <th>actor_1</th>\n",
       "      <th>assoc_actor_1</th>\n",
       "      <th>actor_2</th>\n",
       "      <th>assoc_actor_2</th>\n",
       "      <th>location</th>\n",
       "      <th>event_text</th>\n",
       "      <th>event_text_clean</th>\n",
       "      <th>event_text_tokenize</th>\n",
       "      <th>event_text_normalize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10317</td>\n",
       "      <td>2017-08-04</td>\n",
       "      <td>Remote violence</td>\n",
       "      <td>Unidentified Military Forces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thiban</td>\n",
       "      <td>Unknown warplanes targeted the village of Thib...</td>\n",
       "      <td>unknown warplanes targeted the village of thib...</td>\n",
       "      <td>['unknown', 'warplanes', 'targeted', 'the', 'v...</td>\n",
       "      <td>['unknown', 'warplane', 'targeted', 'village',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10300</td>\n",
       "      <td>2017-08-04</td>\n",
       "      <td>Battle-No change of territory</td>\n",
       "      <td>AAS: Ahrar al Sham</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Opposition Rebels (Syria)</td>\n",
       "      <td>Jund al Aqsa</td>\n",
       "      <td>Maar Shamarin</td>\n",
       "      <td>Clashes between Ahrar al-Sham militia and mili...</td>\n",
       "      <td>clashes between ahrar al sham militia and mili...</td>\n",
       "      <td>['clashes', 'between', 'ahrar', 'al', 'sham', ...</td>\n",
       "      <td>['clash', 'ahrar', 'al', 'sham', 'militia', 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10283</td>\n",
       "      <td>2017-08-04</td>\n",
       "      <td>Remote violence</td>\n",
       "      <td>Islamist Rebels (Syria)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Military Forces of Syria (2000-)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bashkwi</td>\n",
       "      <td>The Islamic rebel troops targeted Syrian army ...</td>\n",
       "      <td>the islamic rebel troops targeted syrian army ...</td>\n",
       "      <td>['the', 'islamic', 'rebel', 'troops', 'targete...</td>\n",
       "      <td>['islamic', 'rebel', 'troop', 'targeted', 'syr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10318</td>\n",
       "      <td>2017-08-04</td>\n",
       "      <td>Remote violence</td>\n",
       "      <td>Military Forces of Syria (2000-)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Um Hartein</td>\n",
       "      <td>The Syrian army shelled the villages of Murak,...</td>\n",
       "      <td>the syrian army shelled the villages of murak ...</td>\n",
       "      <td>['the', 'syrian', 'army', 'shelled', 'the', 'v...</td>\n",
       "      <td>['syrian', 'army', 'shelled', 'village', 'mura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10319</td>\n",
       "      <td>2017-08-04</td>\n",
       "      <td>Remote violence</td>\n",
       "      <td>Unidentified Armed Group (Syria)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HTS: Hayat Tahrir al Sham</td>\n",
       "      <td>Civilians (Syria)</td>\n",
       "      <td>Urum al-Kubra</td>\n",
       "      <td>Two HTS members and 2 civilians were killed in...</td>\n",
       "      <td>two hts members and 2 civilians were killed in...</td>\n",
       "      <td>['two', 'hts', 'members', 'and', '2', 'civilia...</td>\n",
       "      <td>['two', 'hts', 'member', '2', 'civilian', 'kil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_id  event_date                     event_type  \\\n",
       "0     10317  2017-08-04                Remote violence   \n",
       "1     10300  2017-08-04  Battle-No change of territory   \n",
       "2     10283  2017-08-04                Remote violence   \n",
       "3     10318  2017-08-04                Remote violence   \n",
       "4     10319  2017-08-04                Remote violence   \n",
       "\n",
       "                            actor_1 assoc_actor_1  \\\n",
       "0      Unidentified Military Forces           NaN   \n",
       "1                AAS: Ahrar al Sham           NaN   \n",
       "2           Islamist Rebels (Syria)           NaN   \n",
       "3  Military Forces of Syria (2000-)           NaN   \n",
       "4  Unidentified Armed Group (Syria)           NaN   \n",
       "\n",
       "                            actor_2      assoc_actor_2       location  \\\n",
       "0                               NaN                NaN         Thiban   \n",
       "1         Opposition Rebels (Syria)       Jund al Aqsa  Maar Shamarin   \n",
       "2  Military Forces of Syria (2000-)                NaN        Bashkwi   \n",
       "3                               NaN                NaN     Um Hartein   \n",
       "4         HTS: Hayat Tahrir al Sham  Civilians (Syria)  Urum al-Kubra   \n",
       "\n",
       "                                          event_text  \\\n",
       "0  Unknown warplanes targeted the village of Thib...   \n",
       "1  Clashes between Ahrar al-Sham militia and mili...   \n",
       "2  The Islamic rebel troops targeted Syrian army ...   \n",
       "3  The Syrian army shelled the villages of Murak,...   \n",
       "4  Two HTS members and 2 civilians were killed in...   \n",
       "\n",
       "                                    event_text_clean  \\\n",
       "0  unknown warplanes targeted the village of thib...   \n",
       "1  clashes between ahrar al sham militia and mili...   \n",
       "2  the islamic rebel troops targeted syrian army ...   \n",
       "3  the syrian army shelled the villages of murak ...   \n",
       "4  two hts members and 2 civilians were killed in...   \n",
       "\n",
       "                                 event_text_tokenize  \\\n",
       "0  ['unknown', 'warplanes', 'targeted', 'the', 'v...   \n",
       "1  ['clashes', 'between', 'ahrar', 'al', 'sham', ...   \n",
       "2  ['the', 'islamic', 'rebel', 'troops', 'targete...   \n",
       "3  ['the', 'syrian', 'army', 'shelled', 'the', 'v...   \n",
       "4  ['two', 'hts', 'members', 'and', '2', 'civilia...   \n",
       "\n",
       "                                event_text_normalize  \n",
       "0  ['unknown', 'warplane', 'targeted', 'village',...  \n",
       "1  ['clash', 'ahrar', 'al', 'sham', 'militia', 'm...  \n",
       "2  ['islamic', 'rebel', 'troop', 'targeted', 'syr...  \n",
       "3  ['syrian', 'army', 'shelled', 'village', 'mura...  \n",
       "4  ['two', 'hts', 'member', '2', 'civilian', 'kil...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Tf-idf?\n",
    "Tf-idf is a very common technique for determining roughly what each document in a set of documents is “about”. It cleverly accomplishes this by looking at two simple metrics: tf (term frequency) and idf (inverse document frequency). Term frequency is the proportion of occurrences of a specific term to total number of terms in a document. Inverse document frequency is the inverse of the proportion of documents that contain that word/phrase. Simple, right!? The general idea is that if a specific phrase appears a lot of times in a given document, but it doesn’t appear in many other documents, then we have a good idea that the phrase is important in distinguishing that document from all the others. Let’s think about it a bit more concretely:\n",
    "\n",
    "If the word \"nails\" show up 5 times in the document we're looking at, then that's pretty different if there are 100 total words in the document or 10,000. The latter document mentions nails but doesn't seem to be significantly about nails (this is why Term Frequency is a proportion instead of a raw count)\n",
    "If the word \"nails\" shows up in 1% of all documents, that's pretty different than if it shows up in 80% of all documents. In the latter case, it's less unique to the document we're looking at.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf is a transformation you apply to texts to get two real-valued vectors. You can then obtain the cosine similarity of any pair of vectors by taking their dot product and dividing that by the product of their norms. That yields the cosine of the angle between the vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the difference between TfidfVectorizer and CountVectorizer?\n",
    "\n",
    "TfidfVectorizer combines all options of CountVectorizer and TfidfTransformer in a single model.\n",
    "\n",
    "CountVectorizer just counts the word frequencies. Simple as that.\n",
    "\n",
    "With the TFIDFVectorizer the value increases proportionally to count, but is offset by the frequency of the word in the corpus. - This is the IDF (inverse document frequency part)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_test = tweets_df.copy()\n",
    "tweet_test = tweet_test[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_test = events_df.copy()\n",
    "event_test = event_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_test['tweet_text_normalize'] = tweet_test['tweet_text_normalize'].apply(lambda x: ast.literal_eval(x))\n",
    "event_test['event_text_normalize'] = event_test['event_text_normalize'].apply(lambda x: ast.literal_eval(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_test_vec = tweet_test.tweet_text_normalize.tolist()\n",
    "event_test_vec = event_test.event_text_normalize.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_test_vec = [ ' '.join(x) for x in tweet_test_vec ]\n",
    "event_test_vec = [ ' '.join(x) for x in event_test_vec ]\n",
    "print(tweet_test_vec[:10])\n",
    "print(event_test_vec[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The difference between fit_transform(), fit(), and transform()\n",
    "Hence, every sklearn's transform's fit() just calculates the parameters (e.g. μ and σ in case of StandardScaler) and saves them as an internal objects state. Afterwards, you can call its transform() method to apply the transformation to a particular set of examples.\n",
    "\n",
    "fit_transform() joins these two steps and is used for the initial fitting of parameters on the training set x, but it also returns a transformed x′. Internally, it just calls first fit() and then transform() on the same data.\n",
    "\n",
    "\n",
    "In layman's terms, fit_transform means to do some calculation and then do transformation (say calculating the means of columns from some data and then replacing the missing values). So for training set, you need to both calculate and do transformation.\n",
    "\n",
    "But for testing set (event set), Machine learning applies prediction based on what was learned during the training set and so it doesn't need to calculate, it just performs the transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explaination #2\n",
    "So by fit the imputer calculates the means of columns from some data, and by transform it applies those means to some data (which is just replacing missing values with the means). If both these data are the same (i.e. the data for calculating the means and the data that means are applied to) you can use fit_transform which is basically a fit followed by a transform.\n",
    "\n",
    "Now your questions:\n",
    "\n",
    "Why we might need to transform data?\n",
    "\n",
    "\"For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical\" (source)\n",
    "\n",
    "What does it mean fitting model on training data and transforming to test data?\n",
    "\n",
    "The fit of an imputer has nothing to do with fit used in model fitting. So using imputer's fit on training data just calculates means of each column of training data. Using transform on test data then replaces missing values of test data with means that were calculated from training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "\n",
    "'''remove punctuation, lowercase, stem'''\n",
    "def normalize(text):\n",
    "    t = stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
    "    return t\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')\n",
    "\n",
    "def cosine_sim(text1, text2):\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    return ((tfidf * tfidf.T).A)[0,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "tweet_set = tweet_test_vec #Tweets\n",
    "event_set = event_test_vec #Event Query\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "trainVectorizerArray = vectorizer.fit_transform(tweet_set).toarray()\n",
    "testVectorizerArray = vectorizer.fit_transform(event_set).toarray()\n",
    "\n",
    "\n",
    "# print (\"cosine scores ==> \")\n",
    "# cosine_similarity(trainVectorizerArray[0:1], testVectorizerArray)  #here the first element of tfidf_matrix_train is matched with other three elements\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "\n",
    "tweet_set = tweet_test_vec #Tweets\n",
    "event_set = event_test_vec #Event Query\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "transformer = TfidfTransformer()\n",
    "\n",
    "trainVectorizerArray = vectorizer.fit_transform(tweet_set).toarray()\n",
    "testVectorizerArray = vectorizer.transform(event_set).toarray()\n",
    "\n",
    "cx = lambda a, b : round(np.inner(a, b)/(LA.norm(a)*LA.norm(b)), 3)\n",
    "# for train_vector in trainVectorizerArray:\n",
    "#     print('Tweet')\n",
    "#     for test_vector in testVectorizerArray:\n",
    "        \n",
    "#         cosine = cx(train_vector, test_vector)\n",
    "#         print (cosine)\n",
    "\n",
    "# transformer.fit(trainVectorizerArray)\n",
    "# print (transformer.transform(trainVectorizerArray).toarray())\n",
    "\n",
    "# transformer.fit(testVectorizerArray)\n",
    "# tfidf = transformer.transform(testVectorizerArray)\n",
    "# print (tfidf.todense())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy.linalg as LA\n",
    "\n",
    "tweet_set = tweet_test_vec #Tweets\n",
    "event_set = event_test_vec #Event Query\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tweetVectorizerArray = vectorizer.fit_transform(tweet_set).toarray()\n",
    "eventVectorizerArray = vectorizer.transform(event_set).toarray()\n",
    "\n",
    "cx = lambda a, b : round(np.inner(a, b)/(LA.norm(a)*LA.norm(b)), 3)\n",
    "test = []\n",
    "for tweet_vector in tweetVectorizerArray:\n",
    "    cosine_list = []\n",
    "    for event_vector in eventVectorizerArray:\n",
    "        cosine = cx(tweet_vector, event_vector)\n",
    "        cosine_list.append(cosine)\n",
    "#         print (cosine)\n",
    "    test.append(max(cosine_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.sort(reverse=True)\n",
    "print(test[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
