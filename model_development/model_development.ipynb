{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import core libraries \n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import ast\n",
    "import pprint\n",
    "import pathlib\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "\n",
    "# import third-party libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import visualizations\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory path data\n",
    "syria_data_dir = pathlib.Path('/Users/adamstueckrath/Desktop/syria_data/')\n",
    "\n",
    "# syria_events_csv file path\n",
    "events_pre_processed_csv = syria_data_dir / 'model' / 'model_data' / 'events_pre_processed.csv'\n",
    "\n",
    "# tweets_no_rts_csv file path\n",
    "tweets_pre_processed_csv = syria_data_dir / 'model' / 'model_data' / 'tweets_pre_processed.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv(tweets_pre_processed_csv, header=0)\n",
    "events_df = pd.read_csv(events_pre_processed_csv, header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tweets_df.dropna(subset=['tweet_text_normalize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_id_str</th>\n",
       "      <th>tweet_created_at</th>\n",
       "      <th>tweet_lang</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>user_id_str</th>\n",
       "      <th>user_name</th>\n",
       "      <th>tweet_text_clean</th>\n",
       "      <th>tweet_sentiment_compound</th>\n",
       "      <th>tweet_sentiment_score</th>\n",
       "      <th>tweet_sentiment_label</th>\n",
       "      <th>tweet_text_tokenize</th>\n",
       "      <th>tweet_text_normalize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>595e82d713bbf01307babbbd</td>\n",
       "      <td>8.830315e+17</td>\n",
       "      <td>2017-07-06 18:34:41</td>\n",
       "      <td>en</td>\n",
       "      <td>@Autumblues @JudeStevens @CBCNews So Syria and...</td>\n",
       "      <td>1.363324e+08</td>\n",
       "      <td>Gab.ai ➡</td>\n",
       "      <td>so syria and libya is on who but lets hear the...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>['so', 'syria', 'and', 'libya', 'is', 'on', 'w...</td>\n",
       "      <td>['syria', 'libya', 'let', 'hear', 'excuse', '4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>595e82d713bbf01307babbc6</td>\n",
       "      <td>8.830315e+17</td>\n",
       "      <td>2017-07-06 18:34:53</td>\n",
       "      <td>en</td>\n",
       "      <td>@Diane1hDiane SHE'S FAR FROM CLUELESS. UNLIKE ...</td>\n",
       "      <td>7.968346e+17</td>\n",
       "      <td>WakeUP2017</td>\n",
       "      <td>she s far from clueless unlike most including ...</td>\n",
       "      <td>-0.4215</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>['she', 's', 'far', 'from', 'clueless', 'unlik...</td>\n",
       "      <td>['far', 'clueless', 'unlike', 'including', 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>595e82d713bbf01307babbc9</td>\n",
       "      <td>8.830315e+17</td>\n",
       "      <td>2017-07-06 18:34:54</td>\n",
       "      <td>en</td>\n",
       "      <td>See footage from the #G20 #WelcomeToHell prote...</td>\n",
       "      <td>2.429008e+09</td>\n",
       "      <td>Vero Ger</td>\n",
       "      <td>see footage from the g20 welcometohell protest...</td>\n",
       "      <td>-0.2263</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>['see', 'footage', 'from', 'the', 'g20', 'welc...</td>\n",
       "      <td>['see', 'footage', 'g20', 'welcometohell', 'pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>595e82d713bbf01307babbcb</td>\n",
       "      <td>8.830315e+17</td>\n",
       "      <td>2017-07-06 18:34:56</td>\n",
       "      <td>en</td>\n",
       "      <td>Global inquiry aims to report on Syria sarin a...</td>\n",
       "      <td>8.174506e+17</td>\n",
       "      <td>NEWS</td>\n",
       "      <td>global inquiry aims to report on syria sarin a...</td>\n",
       "      <td>-0.4767</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>['global', 'inquiry', 'aims', 'to', 'report', ...</td>\n",
       "      <td>['global', 'inquiry', 'aim', 'report', 'syria'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>595e82d713bbf01307babbcd</td>\n",
       "      <td>8.830315e+17</td>\n",
       "      <td>2017-07-06 18:34:57</td>\n",
       "      <td>en</td>\n",
       "      <td>@TheSwogBlog Bill liked Trump's muh holocaust ...</td>\n",
       "      <td>8.826904e+17</td>\n",
       "      <td>Andrew Saxon</td>\n",
       "      <td>bill liked trump s muh holocaust amp russia sy...</td>\n",
       "      <td>0.1531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>['bill', 'liked', 'trump', 's', 'muh', 'holoca...</td>\n",
       "      <td>['bill', 'liked', 'trump', 'muh', 'holocaust',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   tweet_id  tweet_id_str     tweet_created_at tweet_lang  \\\n",
       "0  595e82d713bbf01307babbbd  8.830315e+17  2017-07-06 18:34:41         en   \n",
       "1  595e82d713bbf01307babbc6  8.830315e+17  2017-07-06 18:34:53         en   \n",
       "2  595e82d713bbf01307babbc9  8.830315e+17  2017-07-06 18:34:54         en   \n",
       "3  595e82d713bbf01307babbcb  8.830315e+17  2017-07-06 18:34:56         en   \n",
       "4  595e82d713bbf01307babbcd  8.830315e+17  2017-07-06 18:34:57         en   \n",
       "\n",
       "                                          tweet_text   user_id_str  \\\n",
       "0  @Autumblues @JudeStevens @CBCNews So Syria and...  1.363324e+08   \n",
       "1  @Diane1hDiane SHE'S FAR FROM CLUELESS. UNLIKE ...  7.968346e+17   \n",
       "2  See footage from the #G20 #WelcomeToHell prote...  2.429008e+09   \n",
       "3  Global inquiry aims to report on Syria sarin a...  8.174506e+17   \n",
       "4  @TheSwogBlog Bill liked Trump's muh holocaust ...  8.826904e+17   \n",
       "\n",
       "      user_name                                   tweet_text_clean  \\\n",
       "0      Gab.ai ➡  so syria and libya is on who but lets hear the...   \n",
       "1    WakeUP2017  she s far from clueless unlike most including ...   \n",
       "2      Vero Ger  see footage from the g20 welcometohell protest...   \n",
       "3          NEWS  global inquiry aims to report on syria sarin a...   \n",
       "4  Andrew Saxon  bill liked trump s muh holocaust amp russia sy...   \n",
       "\n",
       "  tweet_sentiment_compound  tweet_sentiment_score tweet_sentiment_label  \\\n",
       "0                      0.0                    0.0               neutral   \n",
       "1                  -0.4215                   -1.0              negative   \n",
       "2                  -0.2263                   -1.0              negative   \n",
       "3                  -0.4767                   -1.0              negative   \n",
       "4                   0.1531                    0.0               neutral   \n",
       "\n",
       "                                 tweet_text_tokenize  \\\n",
       "0  ['so', 'syria', 'and', 'libya', 'is', 'on', 'w...   \n",
       "1  ['she', 's', 'far', 'from', 'clueless', 'unlik...   \n",
       "2  ['see', 'footage', 'from', 'the', 'g20', 'welc...   \n",
       "3  ['global', 'inquiry', 'aims', 'to', 'report', ...   \n",
       "4  ['bill', 'liked', 'trump', 's', 'muh', 'holoca...   \n",
       "\n",
       "                                tweet_text_normalize  \n",
       "0  ['syria', 'libya', 'let', 'hear', 'excuse', '4...  \n",
       "1  ['far', 'clueless', 'unlike', 'including', 'co...  \n",
       "2  ['see', 'footage', 'g20', 'welcometohell', 'pr...  \n",
       "3  ['global', 'inquiry', 'aim', 'report', 'syria'...  \n",
       "4  ['bill', 'liked', 'trump', 'muh', 'holocaust',...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>event_date</th>\n",
       "      <th>event_type</th>\n",
       "      <th>actor_1</th>\n",
       "      <th>assoc_actor_1</th>\n",
       "      <th>actor_2</th>\n",
       "      <th>assoc_actor_2</th>\n",
       "      <th>location</th>\n",
       "      <th>event_text</th>\n",
       "      <th>event_text_clean</th>\n",
       "      <th>event_text_tokenize</th>\n",
       "      <th>event_text_normalize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10317</td>\n",
       "      <td>2017-08-04</td>\n",
       "      <td>Remote violence</td>\n",
       "      <td>Unidentified Military Forces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thiban</td>\n",
       "      <td>Unknown warplanes targeted the village of Thib...</td>\n",
       "      <td>unknown warplanes targeted the village of thib...</td>\n",
       "      <td>['unknown', 'warplanes', 'targeted', 'the', 'v...</td>\n",
       "      <td>['unknown', 'warplane', 'targeted', 'village',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10300</td>\n",
       "      <td>2017-08-04</td>\n",
       "      <td>Battle-No change of territory</td>\n",
       "      <td>AAS: Ahrar al Sham</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Opposition Rebels (Syria)</td>\n",
       "      <td>Jund al Aqsa</td>\n",
       "      <td>Maar Shamarin</td>\n",
       "      <td>Clashes between Ahrar al-Sham militia and mili...</td>\n",
       "      <td>clashes between ahrar al sham militia and mili...</td>\n",
       "      <td>['clashes', 'between', 'ahrar', 'al', 'sham', ...</td>\n",
       "      <td>['clash', 'ahrar', 'al', 'sham', 'militia', 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10283</td>\n",
       "      <td>2017-08-04</td>\n",
       "      <td>Remote violence</td>\n",
       "      <td>Islamist Rebels (Syria)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Military Forces of Syria (2000-)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bashkwi</td>\n",
       "      <td>The Islamic rebel troops targeted Syrian army ...</td>\n",
       "      <td>the islamic rebel troops targeted syrian army ...</td>\n",
       "      <td>['the', 'islamic', 'rebel', 'troops', 'targete...</td>\n",
       "      <td>['islamic', 'rebel', 'troop', 'targeted', 'syr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10318</td>\n",
       "      <td>2017-08-04</td>\n",
       "      <td>Remote violence</td>\n",
       "      <td>Military Forces of Syria (2000-)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Um Hartein</td>\n",
       "      <td>The Syrian army shelled the villages of Murak,...</td>\n",
       "      <td>the syrian army shelled the villages of murak ...</td>\n",
       "      <td>['the', 'syrian', 'army', 'shelled', 'the', 'v...</td>\n",
       "      <td>['syrian', 'army', 'shelled', 'village', 'mura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10319</td>\n",
       "      <td>2017-08-04</td>\n",
       "      <td>Remote violence</td>\n",
       "      <td>Unidentified Armed Group (Syria)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HTS: Hayat Tahrir al Sham</td>\n",
       "      <td>Civilians (Syria)</td>\n",
       "      <td>Urum al-Kubra</td>\n",
       "      <td>Two HTS members and 2 civilians were killed in...</td>\n",
       "      <td>two hts members and 2 civilians were killed in...</td>\n",
       "      <td>['two', 'hts', 'members', 'and', '2', 'civilia...</td>\n",
       "      <td>['two', 'hts', 'member', '2', 'civilian', 'kil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_id  event_date                     event_type  \\\n",
       "0     10317  2017-08-04                Remote violence   \n",
       "1     10300  2017-08-04  Battle-No change of territory   \n",
       "2     10283  2017-08-04                Remote violence   \n",
       "3     10318  2017-08-04                Remote violence   \n",
       "4     10319  2017-08-04                Remote violence   \n",
       "\n",
       "                            actor_1 assoc_actor_1  \\\n",
       "0      Unidentified Military Forces           NaN   \n",
       "1                AAS: Ahrar al Sham           NaN   \n",
       "2           Islamist Rebels (Syria)           NaN   \n",
       "3  Military Forces of Syria (2000-)           NaN   \n",
       "4  Unidentified Armed Group (Syria)           NaN   \n",
       "\n",
       "                            actor_2      assoc_actor_2       location  \\\n",
       "0                               NaN                NaN         Thiban   \n",
       "1         Opposition Rebels (Syria)       Jund al Aqsa  Maar Shamarin   \n",
       "2  Military Forces of Syria (2000-)                NaN        Bashkwi   \n",
       "3                               NaN                NaN     Um Hartein   \n",
       "4         HTS: Hayat Tahrir al Sham  Civilians (Syria)  Urum al-Kubra   \n",
       "\n",
       "                                          event_text  \\\n",
       "0  Unknown warplanes targeted the village of Thib...   \n",
       "1  Clashes between Ahrar al-Sham militia and mili...   \n",
       "2  The Islamic rebel troops targeted Syrian army ...   \n",
       "3  The Syrian army shelled the villages of Murak,...   \n",
       "4  Two HTS members and 2 civilians were killed in...   \n",
       "\n",
       "                                    event_text_clean  \\\n",
       "0  unknown warplanes targeted the village of thib...   \n",
       "1  clashes between ahrar al sham militia and mili...   \n",
       "2  the islamic rebel troops targeted syrian army ...   \n",
       "3  the syrian army shelled the villages of murak ...   \n",
       "4  two hts members and 2 civilians were killed in...   \n",
       "\n",
       "                                 event_text_tokenize  \\\n",
       "0  ['unknown', 'warplanes', 'targeted', 'the', 'v...   \n",
       "1  ['clashes', 'between', 'ahrar', 'al', 'sham', ...   \n",
       "2  ['the', 'islamic', 'rebel', 'troops', 'targete...   \n",
       "3  ['the', 'syrian', 'army', 'shelled', 'the', 'v...   \n",
       "4  ['two', 'hts', 'members', 'and', '2', 'civilia...   \n",
       "\n",
       "                                event_text_normalize  \n",
       "0  ['unknown', 'warplane', 'targeted', 'village',...  \n",
       "1  ['clash', 'ahrar', 'al', 'sham', 'militia', 'm...  \n",
       "2  ['islamic', 'rebel', 'troop', 'targeted', 'syr...  \n",
       "3  ['syrian', 'army', 'shelled', 'village', 'mura...  \n",
       "4  ['two', 'hts', 'member', '2', 'civilian', 'kil...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Tf-idf?\n",
    "Tf-idf is a very common technique for determining roughly what each document in a set of documents is “about”. It cleverly accomplishes this by looking at two simple metrics: tf (term frequency) and idf (inverse document frequency). Term frequency is the proportion of occurrences of a specific term to total number of terms in a document. Inverse document frequency is the inverse of the proportion of documents that contain that word/phrase. Simple, right!? The general idea is that if a specific phrase appears a lot of times in a given document, but it doesn’t appear in many other documents, then we have a good idea that the phrase is important in distinguishing that document from all the others. Let’s think about it a bit more concretely:\n",
    "\n",
    "If the word \"nails\" show up 5 times in the document we're looking at, then that's pretty different if there are 100 total words in the document or 10,000. The latter document mentions nails but doesn't seem to be significantly about nails (this is why Term Frequency is a proportion instead of a raw count)\n",
    "If the word \"nails\" shows up in 1% of all documents, that's pretty different than if it shows up in 80% of all documents. In the latter case, it's less unique to the document we're looking at.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf is a transformation you apply to texts to get two real-valued vectors. You can then obtain the cosine similarity of any pair of vectors by taking their dot product and dividing that by the product of their norms. That yields the cosine of the angle between the vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the difference between TfidfVectorizer and CountVectorizer?\n",
    "\n",
    "TfidfVectorizer combines all options of CountVectorizer and TfidfTransformer in a single model.\n",
    "\n",
    "CountVectorizer just counts the word frequencies. Simple as that.\n",
    "\n",
    "With the TFIDFVectorizer the value increases proportionally to count, but is offset by the frequency of the word in the corpus. - This is the IDF (inverse document frequency part)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_test = tweets_df.copy()\n",
    "tweet_test = tweet_test[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_test = events_df.copy()\n",
    "event_test = event_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_test['tweet_text_normalize'] = tweet_test['tweet_text_normalize'].apply(lambda x: ast.literal_eval(x))\n",
    "event_test['event_text_normalize'] = event_test['event_text_normalize'].apply(lambda x: ast.literal_eval(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_test_vec = tweet_test.tweet_text_normalize.tolist()\n",
    "event_test_vec = event_test.event_text_normalize.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['syria libya let hear excuse 4 3 2 1 go', 'far clueless unlike including congress amp fakenews spent serious time', 'see footage g20 welcometohell protest hamburg germany syria syrie', 'global inquiry aim report syria sarin attack october world', 'bill liked trump muh holocaust amp russia syria finger wagging vox butthurt fawning', 'pursuing hard truth syria v western medium war propaganda', 'pursuing hard truth syria v western medium war propaganda', 'maybe tell everyone want come would like start syria', 'essence someone syria board raft float buy pack gum poof c', 'much revolution r anarchism user happy kurdish fighter syria really really hoped fello']\n",
      "['unknown warplane targeted village thiban deir ez zor countryside air strike fatality reported', 'clash ahrar al sham militia militant previously belonged jund al aqsa militia took place maar shamarin village idleb countryside fatality reported', 'islamic rebel troop targeted syrian army location bashkwi area north aleppo city fatality reported regime side unknown number fatality coded 10', 'syrian army shelled village murak lahaya atshan um hartein north eastern countryside hama fatality reported', 'two hts member 2 civilian killed car explosion targeted command center hts reef al muhandiseen area vicinity urum al kubra village', 'protest syrian government took place talbiseh town homs countryside fatality reported', 'rebel troop clashed syrian democratic force rebel troop attempted advance towards ein daqneh village azaz district aleppo northern countryside neither fatality ground shifting reported', 'syrian army shelled talbiseh village northern countryside homs led killing ofing one militant islamic rebel troop', 'syrian army ally clashed islamic state north al heel oil field countryside sokhneh homs province fatality reported', 'unknown warplane targeted village suha hama countryside fatality reported']\n"
     ]
    }
   ],
   "source": [
    "tweet_test_vec = [ ' '.join(x) for x in tweet_test_vec ]\n",
    "event_test_vec = [ ' '.join(x) for x in event_test_vec ]\n",
    "print(tweet_test_vec[:10])\n",
    "print(event_test_vec[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The difference between fit_transform(), fit(), and transform()\n",
    "Hence, every sklearn's transform's fit() just calculates the parameters (e.g. μ and σ in case of StandardScaler) and saves them as an internal objects state. Afterwards, you can call its transform() method to apply the transformation to a particular set of examples.\n",
    "\n",
    "fit_transform() joins these two steps and is used for the initial fitting of parameters on the training set x, but it also returns a transformed x′. Internally, it just calls first fit() and then transform() on the same data.\n",
    "\n",
    "\n",
    "In layman's terms, fit_transform means to do some calculation and then do transformation (say calculating the means of columns from some data and then replacing the missing values). So for training set, you need to both calculate and do transformation.\n",
    "\n",
    "But for testing set (event set), Machine learning applies prediction based on what was learned during the training set and so it doesn't need to calculate, it just performs the transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explaination #2\n",
    "So by fit the imputer calculates the means of columns from some data, and by transform it applies those means to some data (which is just replacing missing values with the means). If both these data are the same (i.e. the data for calculating the means and the data that means are applied to) you can use fit_transform which is basically a fit followed by a transform.\n",
    "\n",
    "Now your questions:\n",
    "\n",
    "Why we might need to transform data?\n",
    "\n",
    "\"For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical\" (source)\n",
    "\n",
    "What does it mean fitting model on training data and transforming to test data?\n",
    "\n",
    "The fit of an imputer has nothing to do with fit used in model fitting. So using imputer's fit on training data just calculates means of each column of training data. Using transform on test data then replaces missing values of test data with means that were calculated from training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "\n",
    "'''remove punctuation, lowercase, stem'''\n",
    "def normalize(text):\n",
    "    t = stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
    "    return t\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')\n",
    "\n",
    "def cosine_sim(text1, text2):\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    return ((tfidf * tfidf.T).A)[0,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "tweet_set = tweet_test_vec #Tweets\n",
    "event_set = event_test_vec #Event Query\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "trainVectorizerArray = vectorizer.fit_transform(tweet_set).toarray()\n",
    "testVectorizerArray = vectorizer.fit_transform(event_set).toarray()\n",
    "\n",
    "\n",
    "# print (\"cosine scores ==> \")\n",
    "# cosine_similarity(trainVectorizerArray[0:1], testVectorizerArray)  #here the first element of tfidf_matrix_train is matched with other three elements\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "\n",
    "tweet_set = tweet_test_vec #Tweets\n",
    "event_set = event_test_vec #Event Query\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "transformer = TfidfTransformer()\n",
    "\n",
    "trainVectorizerArray = vectorizer.fit_transform(tweet_set).toarray()\n",
    "testVectorizerArray = vectorizer.transform(event_set).toarray()\n",
    "\n",
    "cx = lambda a, b : round(np.inner(a, b)/(LA.norm(a)*LA.norm(b)), 3)\n",
    "# for train_vector in trainVectorizerArray:\n",
    "#     print('Tweet')\n",
    "#     for test_vector in testVectorizerArray:\n",
    "        \n",
    "#         cosine = cx(train_vector, test_vector)\n",
    "#         print (cosine)\n",
    "\n",
    "# transformer.fit(trainVectorizerArray)\n",
    "# print (transformer.transform(trainVectorizerArray).toarray())\n",
    "\n",
    "# transformer.fit(testVectorizerArray)\n",
    "# tfidf = transformer.transform(testVectorizerArray)\n",
    "# print (tfidf.todense())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['many', 'civilian', 'injured', 'cannon', 'attack', 'alleramoon', 'town', 'aleppo', 'countryside', 'aleppo', 'syria', 'civilian']\n",
      "['ahl', 'diya', 'operation', 'room', 'attack', 'ein', 'daqneh', 'village', 'aleppo', 'repelled', 'ypg', 'qsd', 'force', 'neither', 'injury', 'fatality', 'reported']\n"
     ]
    }
   ],
   "source": [
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy.linalg as LA\n",
    "\n",
    "# algo \n",
    "tweet_set = tweet_test_vec[1290].split() #tweets\n",
    "event_set = event_test_vec[41].split() # event query\n",
    "print(tweet_set)\n",
    "print(event_set)\n",
    "vectorizer = TfidfVectorizer()\n",
    "tweetVectorizerArray = vectorizer.fit_transform(tweet_set).toarray()\n",
    "eventVectorizerArray = vectorizer.transform(event_set).toarray()\n",
    "\n",
    "event_id_list = events_df.event_id.tolist()\n",
    "tweet_event_ids = []\n",
    "cosine_x = lambda a, b : round(np.inner(a, b)/(LA.norm(a)*LA.norm(b)), 3)\n",
    "\n",
    "for tweet_vector in tweetVectorizerArray:\n",
    "    cosine_dict = dict()\n",
    "    for event_id, event_vector in zip(event_id_list, eventVectorizerArray):\n",
    "        cosine = cosine_x(tweet_vector, event_vector)\n",
    "        cosine_dict[event_id] = cosine\n",
    "    tweet_event_ids.append(max(cosine_dict, key=cosine_dict.get))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10317\n"
     ]
    }
   ],
   "source": [
    "# assign event_ids to tweets\n",
    "print(tweet_event_ids[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
